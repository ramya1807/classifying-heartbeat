{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import numpy as np\n",
        "import random\n",
        "import csv\n",
        "import soundfile\n",
        "\n",
        "import IPython\n",
        "from scipy.io import wavfile\n",
        "import noisereduce as nr\n",
        "import soundfile as sf\n",
        "from noisereduce.generate_noise import band_limited_noise\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import io\n",
        "\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, LSTM\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint,TensorBoard,ProgbarLogger\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn import metrics \n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import itertools"
      ],
      "metadata": {
        "id": "ZCVF2B8sZYsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%% load dataset info\n",
        "# a_data = pd.read_csv(\"/content/set_a.csv\")\n",
        "# b_data = pd.read_csv(\"/content/set_b.csv\")\n",
        "a_data = pd.read_csv(\"set_a.csv\")\n",
        "b_data = pd.read_csv(\"set_b.csv\")"
      ],
      "metadata": {
        "id": "ekEu-aQsZ-WU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%% merge data as maindata\n",
        "data1 = [a_data, b_data]\n",
        "maindata = pd.concat(data1)\n",
        "maindata.drop([\"sublabel\",\"dataset\"],axis=\"columns\",inplace=True)\n",
        "maindata = maindata.dropna()"
      ],
      "metadata": {
        "id": "68RFlG3dZxuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%% standardize filename\n",
        "maindata = maindata.reset_index()\n",
        "maindata.drop(\"index\",axis=\"columns\",inplace=True)\n",
        "\n",
        "for index, row in maindata.iterrows():\n",
        "  if index >= 124:\n",
        "    maindata.at[index,'fname'] = maindata.at[index,'fname'][:6]+ maindata.at[index,'fname'][16:]\n",
        "    \n",
        "for index, row in maindata.iterrows():\n",
        "  if index >= 436:\n",
        "    maindata.at[index,'fname'] = maindata.at[index,'fname'][:12]+ maindata.at[index,'fname'][22:]\n",
        "\n",
        "for index, row in maindata.iterrows():\n",
        "  if index >= 124 and index <170:\n",
        "    maindata.at[index,'fname'] = maindata.at[index,'fname'][:16]+\"_\"+ maindata.at[index,'fname'][16:]\n",
        "\n",
        "for index, row in maindata.iterrows():\n",
        "  if index >= 170 and index <236:\n",
        "    maindata.at[index,'fname'] = maindata.at[index,'fname'][:12]+\"_\"+ maindata.at[index,'fname'][12:]\n",
        "    \n",
        "for index, row in maindata.iterrows():\n",
        "  if index >= 236 and index <436:\n",
        "    maindata.at[index,'fname'] = maindata.at[index,'fname'][:12]+\"_\"+ maindata.at[index,'fname'][12:]"
      ],
      "metadata": {
        "id": "JTFTOxH05EvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%% Save in a clean MainData.csv with new fname\n",
        "f = open(\"MainData.csv\", \"w\")\n",
        "f.truncate()\n",
        "f.close()\n",
        "\n",
        "write_data = []\n",
        "for index,row in maindata.iterrows():\n",
        "    write_data.append(row)\n",
        "with open(\"MainData.csv\", 'w', newline=\"\") as f: \n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(write_data)"
      ],
      "metadata": {
        "id": "8uCYbIqL5IFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%\n",
        "#testfile = \"set_a/artifact__201012172012.wav\"\n",
        "\n",
        "def write_new_wav(file, audio_signal,sampling_rate=44100):\n",
        "    # audio_signal, sampling_rate = soundfile.read(file)\n",
        "    sf.write(file,audio_signal, sampling_rate)\n",
        "\n",
        "#%%\n",
        "def noise_removal(file):\n",
        "    noise_data, noise_rate = sf.read(file)\n",
        "    snr = 2 # signal to noise ratio\n",
        "    noise_clip = noise_data/snr\n",
        "    audio_clip_cafe = noise_data + noise_clip\n",
        "\n",
        "    reduced_noise = nr.reduce_noise(y = audio_clip_cafe, sr=noise_rate, y_noise = noise_clip,stationary=True)\n",
        "    return reduced_noise\n",
        "\n",
        "#%% MFCCs extraction, input dim = 40\n",
        "def export_function(path, duration=12, sr=16000):\n",
        "  input_length=sr*duration\n",
        "  data, sr = librosa.load(path, res_type='kaiser_fast')\n",
        "  dur = librosa.get_duration(data, sr)\n",
        "  \n",
        "  # pad audio file same duration\n",
        "  if (round(dur) < duration):\n",
        "      #print (\"fixing audio lenght :\", path)\n",
        "      data = librosa.util.fix_length(data, input_length)  \n",
        "  \n",
        "  mfccs = np.mean(librosa.feature.mfcc(data, sr, n_mfcc=40).T,axis=0) \n",
        "  mfccs1 = np.array(mfccs).reshape([-1,1])\n",
        "  return mfccs1"
      ],
      "metadata": {
        "id": "oZ7Rzgwy5K5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%% Division of dataset  // To RESHUFFLE START FROM HERE\n",
        "# x: audio data as nparray(float32), y: labels as str\n",
        "x_Train = []\n",
        "y_Train = []\n",
        "x_Val = []\n",
        "y_Val = []\n",
        "x_Test = []\n",
        "y_Test = []\n",
        "\n",
        "# =============================================================================\n",
        "# set a and b\n",
        "'''\n",
        "total_num = 585\n",
        "# we use roughly 80% for training and 10% each for validation and testing\n",
        "training_num = 465\n",
        "val_num = 60\n",
        "test_num = 60\n",
        "'''\n",
        "# =============================================================================\n",
        "# set_b only\n",
        "total_num = 461\n",
        "# we use roughly 80% for training and 10% each for validation and testing\n",
        "training_num = 369\n",
        "val_num = 46\n",
        "test_num = 46\n",
        "\n",
        "training_list = random.sample(range(125,585),training_num)\n",
        "val_test_list = list(range(125,585))\n",
        "\n",
        "for i in training_list:\n",
        "    val_test_list.remove(i)\n",
        "    \n",
        "random.shuffle(val_test_list)\n",
        "val_list = val_test_list[:46]\n",
        "test_list = val_test_list[46:]\n"
      ],
      "metadata": {
        "id": "slh7JwYQ5Wt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%   \n",
        "# CLASSES = ['murmur','normal','artifact','extrahls','extrastole']\n",
        "CLASSES = ['murmur','normal','extrastole']\n",
        "# Map integer value to text labels to save labels as int\n",
        "label_to_int = {k:v for v,k in enumerate(CLASSES)}\n",
        "\n",
        "counter = 0\n",
        "\n",
        "for path,label in zip(maindata.fname,maindata.label): \n",
        "    \n",
        "  if(counter>123):\n",
        "      if(counter in training_list):\n",
        "          output = export_function(path) #mfccs\n",
        "          x_Train.append(output)\n",
        "          y_Train.append(label_to_int[label])\n",
        "      elif(counter in val_list):\n",
        "          output = export_function(path) #mfccs\n",
        "          x_Val.append(output)\n",
        "          y_Val.append(label_to_int[label])\n",
        "      else: \n",
        "          output = export_function(path) #mfccs\n",
        "          x_Test.append(output)\n",
        "          y_Test.append(label_to_int[label])\n",
        "      \n",
        "  counter += 1\n"
      ],
      "metadata": {
        "id": "LyhNUUhq5Zqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%% Model structure\n",
        "print('Build LSTM RNN model ...')\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=64, dropout=0.05, recurrent_dropout=0.20, return_sequences=True,input_shape = (40,1)))\n",
        "model.add(LSTM(units=32, dropout=0.05, recurrent_dropout=0.20, return_sequences=False))\n",
        "model.add(Dense(len(CLASSES), activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='Adamax', metrics=['acc','mse', 'mae', 'mape'])\n",
        "model.summary()\n",
        "\n",
        "# name of model: lstmModel.hdf5\n",
        "model.save('your_path/lstmModel.hdf5')"
      ],
      "metadata": {
        "id": "RlMj_lx25c4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%\n",
        "# convert labels from list to ndarray\n",
        "y_train = np.array(to_categorical(y_Train, len(CLASSES)))\n",
        "y_test = np.array(to_categorical(y_Test, len(CLASSES)))\n",
        "y_val = np.array(to_categorical(y_Val, len(CLASSES)))\n",
        "\n",
        "# reshape audio data into 3d array\n",
        "x_train = np.reshape(x_Train,(369,40,1))\n",
        "x_val = np.reshape(x_Val,(46,40,1))\n",
        "x_test = np.reshape(x_Test,(46,40,1))"
      ],
      "metadata": {
        "id": "wdRiuAm75jZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%% TRAIN\n",
        "\n",
        "# saved model checkpoint file\n",
        "modelpath=\"your_path/lstmModel.hdf5\"\n",
        "\n",
        "MAX_PATIENT=12\n",
        "MAX_EPOCHS=100\n",
        "MAX_BATCH=32\n",
        "\n",
        "# callbacks\n",
        "callback=[ReduceLROnPlateau(patience=MAX_PATIENT, verbose=1),\n",
        "          ModelCheckpoint(filepath=modelpath, monitor='loss', verbose=1, save_best_only=True)]\n",
        "\n",
        "print (\"Training started..... please wait.\")\n",
        "# training\n",
        "history=model.fit(x_train, y_train, \n",
        "                  batch_size=MAX_BATCH, \n",
        "                  epochs=MAX_EPOCHS,\n",
        "                  verbose=0,\n",
        "                  validation_data=(x_val, y_val),\n",
        "                  callbacks=callback) \n",
        "\n",
        "print (\"Training finised\")"
      ],
      "metadata": {
        "id": "DIP1hfMd5lsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%% TEST\n",
        "# evaluate accuracy againtst all three datasets: training, validtion, testing\n",
        "score = model.evaluate(x_train, y_train, verbose=0) \n",
        "print (\"model training data score       : \",round(score[1]*100) , \"%\")\n",
        "\n",
        "score = model.evaluate(x_val, y_val, verbose=0) \n",
        "print (\"model validation data score     : \", round(score[1]*100), \"%\")\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0) \n",
        "print (\"model testing data score        : \",round(score[1]*100) , \"%\")\n"
      ],
      "metadata": {
        "id": "mVWmCmSR5q73"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}